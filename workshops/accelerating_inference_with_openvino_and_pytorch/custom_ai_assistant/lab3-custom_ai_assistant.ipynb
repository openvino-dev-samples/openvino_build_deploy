{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe1155a-9e14-4c47-bccb-fdb1f2604def",
   "metadata": {},
   "source": [
    "# Lab 3. Custom AI Assistant with OpenVINOâ„¢ Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579e6fb-1dc0-4575-8b98-18686d550de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging as log\n",
    "import time\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "from typing import Tuple, List, Optional, Set\n",
    "\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from optimum.intel import OVModelForCausalLM, OVModelForSpeechSeq2Seq\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor, PreTrainedTokenizer, TextIteratorStreamer\n",
    "from transformers.generation.streamers import BaseStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3907f-671b-4e70-904f-12ca61b492b8",
   "metadata": {},
   "source": [
    "### Convert and Compress ASR model with optimum-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c1e90-8673-4ff6-a273-9d0ecb90d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model_id = \"BELLE-2/Belle-whisper-large-v3-zh\"\n",
    "asr_model_path = \"./model/Belle-whisper-large-v3-zh-ov\"\n",
    "\n",
    "if not Path(asr_model_path).exists():\n",
    "    !optimum-cli export openvino --model {asr_model_id} --task automatic-speech-recognition-with-past --weight-format int8 --trust-remote-code {asr_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ea3b7-4e7b-4782-b35d-50fb4576ad8d",
   "metadata": {},
   "source": [
    "### Convert and Compress LLM model with optimum-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d35c93-6341-4422-a41f-ea084d597579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "llm_model_path = \"./model/Qwen2-7B-Instruct-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    !optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 --trust-remote-code {llm_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e8764-25a6-4d42-8a8e-30e0db792199",
   "metadata": {},
   "source": [
    "### Prepare system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebf4f1-a5b3-4c9a-a98f-bc885ebf6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_AUDIO_SAMPLE_RATE = 16000\n",
    "SYSTEM_CONFIGURATION = (\n",
    "    \"You are Adrishuo - a helpful, respectful, and honest virtual doctor assistant. \"\n",
    "    \"Your role is talking to a patient who just came in.\"\n",
    "    \"Your primary role is to assist in the collection of Symptom information from patients. \"\n",
    "    \"Focus solely on gathering symptom details without offering treatment or medical advice.\"\n",
    "    \"You must only ask follow-up questions based on the patient's initial descriptions to clarify and gather more details about their symtpoms. \"\n",
    "    \"You must not attempt to diagnose, treat, or offer health advice. \"\n",
    "    \"Ask one and only the symptom related followup questions and keep it short. \"\n",
    "    \"You must strictly not suggest or recommend any treatments, including over-the-counter medication. \"\n",
    "    \"You must strictly avoid making any assumptions or conclusions about the causes or nature of the patient's symptoms. \"\n",
    "    \"You must strictly avoid providing suggestions to manage their symptoms. \"\n",
    "    \"Your interactions should be focused solely on understanding and recording the patient's stated symptoms. \"\n",
    "    \"Do not collect or use any personal information like age, name, contact, gender, etc. \"\n",
    "    \"Ask at most 3 questions then say you know everything and you're ready to summarize the patient. \"\n",
    "    \"Remember, your role is to aid in symptom information collection in a supportive, unbiased, and factually accurate manner. \"\n",
    "    \"Your responses should consistently encourage the patient to discuss their symptoms in greater detail while remaining neutral and non-diagnostic.\"\n",
    ")\n",
    "GREET_THE_CUSTOMER = \"Please introduce yourself and greet the patient\"\n",
    "SUMMARIZE_THE_CUSTOMER = (\n",
    "    \"You are now required to summarize the patient's exact provided symptoms for the doctor's review. \"\n",
    "    \"Strictly do not mention any personal data like age, name, gender, contact, non-health information etc. when summarizing.\"\n",
    "    \"Warn the patients for immediate medical seeking in case they exhibit symptoms indicative of critical conditions such as heart attacks, strokes, severe allergic reactions, breathing difficulties, high fever with severe symptoms, significant burns, or severe injuries.\"\n",
    "    \"Summarize the health-related concerns mentioned by the patient in this conversation, focusing only on the information explicitly provided, without adding any assumptions or unrelated symptoms.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f06970-6bc2-4e50-85fe-cc49c35c31dd",
   "metadata": {},
   "source": [
    "### Initialize ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4d999-04c7-4b1a-ad42-1b2e6cdc4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "asr_device = device_widget(default=\"GPU\", exclude=[\"NPU\"])\n",
    "\n",
    "asr_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721aff-5d80-414f-85ba-bba170982584",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = OVModelForSpeechSeq2Seq.from_pretrained(asr_model_path, device=asr_device)\n",
    "asr_processor = AutoProcessor.from_pretrained(asr_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd0715-4056-4180-b60d-365031204a42",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e55787-d3a8-4b44-bd04-ce3dbf7216af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_device = device_widget(default=\"GPU\", exclude=[\"NPU\"])\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a867c-aac0-4a52-bfc0-a3b98e05c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "chat_model = OVModelForCausalLM.from_pretrained(llm_model_path, device=llm_device, config=AutoConfig.from_pretrained(llm_model_path), ov_config=ov_config)\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af60ca2-73bf-42f8-982a-544a8c7d9b7b",
   "metadata": {},
   "source": [
    "### Prepare helper function for chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21657fee-86f2-4593-a5d1-20f8d3d2cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(prompt: str, streamer: BaseStreamer | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Respond to the current prompt\n",
    "\n",
    "    Params:\n",
    "        prompt: user's prompt\n",
    "        streamer: if not None will use it to stream tokens\n",
    "    Returns:\n",
    "        The chat's response\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start time\n",
    "    # tokenize input text\n",
    "    inputs = chat_tokenizer(prompt, return_tensors=\"pt\").to(chat_model.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    # generate response tokens\n",
    "    outputs = chat_model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.6, top_p=0.9, top_k=50, streamer=streamer)\n",
    "    tokens = outputs[0, input_length:]\n",
    "    end_time = time.time()  # End time\n",
    "    log.info(\"Chat model response time: {:.2f} seconds\".format(end_time - start_time))\n",
    "    # decode tokens into text\n",
    "    return chat_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def chat(history: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Chat function. It generates response based on a prompt\n",
    "\n",
    "    Params:\n",
    "        history: history of the messages (conversation) so far\n",
    "    Returns:\n",
    "        History with the latest chat's response (yields partial response)\n",
    "    \"\"\"\n",
    "    # convert list of message to conversation string\n",
    "    conversation = get_conversation(history)\n",
    "\n",
    "    # use streamer to show response word by word\n",
    "    chat_streamer = TextIteratorStreamer(chat_tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # generate response for the conversation in a new thread to deliver response token by token\n",
    "    thread = Thread(target=respond, args=[conversation, chat_streamer])\n",
    "    thread.start()\n",
    "\n",
    "    # get token by token and merge to the final response\n",
    "    history[-1][1] = \"\"\n",
    "    for partial_text in chat_streamer:\n",
    "        history[-1][1] += partial_text\n",
    "        # \"return\" partial response\n",
    "        yield history\n",
    "\n",
    "    # wait for the thread\n",
    "    thread.join()\n",
    "\n",
    "def generate_initial_greeting() -> str:\n",
    "    \"\"\"\n",
    "    Generates customer/patient greeting\n",
    "\n",
    "    Returns:\n",
    "        Generated greeting\n",
    "    \"\"\"\n",
    "    conv = get_conversation([[None, None]])\n",
    "    return respond(conv)\n",
    "\n",
    "def summarize(conversation: List) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the patient case\n",
    "\n",
    "    Params\n",
    "        conversation: history of the messages so far\n",
    "    Returns:\n",
    "        Summary\n",
    "    \"\"\"\n",
    "    conversation.append([SUMMARIZE_THE_CUSTOMER, None])\n",
    "    for partial_summary in chat(conversation):\n",
    "        yield partial_summary[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8b47-0817-4e23-9838-839a1f42687a",
   "metadata": {},
   "source": [
    "### Prepare helper function for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12661202-3f82-4938-9855-1c6816adcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio: Tuple[int, np.ndarray], conversation: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Transcribe audio to text\n",
    "\n",
    "    Params:\n",
    "        audio: audio to transcribe text from\n",
    "        conversation: conversation history with the chatbot\n",
    "    Returns:\n",
    "        User prompt as a text\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start time for ASR process\n",
    "\n",
    "    sample_rate, audio = audio\n",
    "    # the whisper model requires 16000Hz, not 44100Hz\n",
    "    audio = librosa.resample(audio.astype(np.float32), orig_sr=sample_rate, target_sr=TARGET_AUDIO_SAMPLE_RATE).astype(np.int16)\n",
    "\n",
    "    # get input features from the audio\n",
    "    input_features = asr_processor(audio, sampling_rate=TARGET_AUDIO_SAMPLE_RATE, return_tensors=\"pt\").input_features\n",
    "\n",
    "    # use streamer to show transcription word by word\n",
    "    text_streamer = TextIteratorStreamer(asr_processor, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # transcribe in the background to deliver response token by token\n",
    "    thread = Thread(target=asr_model.generate, kwargs={\"input_features\": input_features, \"streamer\": text_streamer})\n",
    "    thread.start()\n",
    "\n",
    "    conversation.append([\"\", None])\n",
    "    # get token by token and merge to the final response\n",
    "    for partial_text in text_streamer:\n",
    "        conversation[-1][0] += partial_text\n",
    "        # \"return\" partial response\n",
    "        yield conversation\n",
    "\n",
    "    end_time = time.time()  # End time for ASR process\n",
    "    log.info(f\"ASR model response time: {end_time - start_time:.2f} seconds\")  # Print the ASR processing time\n",
    "\n",
    "    # wait for the thread\n",
    "    thread.join()\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3d81e-df01-4734-aef3-71786cb31fc1",
   "metadata": {},
   "source": [
    "### Build Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b4e05-5e4e-4453-9c4d-f59c034d498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Talk to Adrishuo - a custom AI assistant working as a healthcare assistant\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Talk to Adrishuo - a custom AI assistant working today as a healthcare assistant\n",
    "\n",
    "    Instructions for use:\n",
    "    - record your question/comment using the first audio widget (\"Your voice input\")\n",
    "    - wait for the chatbot to response (\"Chatbot\")\n",
    "    - click summarize button to make a summary\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        # user's input\n",
    "        input_audio_ui = gr.Audio(sources=[\"microphone\"], scale=5, label=\"Your voice input\")\n",
    "        # submit button\n",
    "        submit_audio_btn = gr.Button(\"Submit\", variant=\"primary\", scale=1, interactive=False)\n",
    "\n",
    "    # chatbot\n",
    "    chatbot_ui = gr.Chatbot(value=[[None, initial_message]], label=\"Chatbot\")\n",
    "\n",
    "    # summarize\n",
    "    summarize_button = gr.Button(\"Summarize\", variant=\"primary\", interactive=False)\n",
    "    summary_ui = gr.Textbox(label=\"Summary\", interactive=False)\n",
    "\n",
    "    # events\n",
    "    # block submit button when no audio input\n",
    "    input_audio_ui.change(lambda x: gr.Button(interactive=False) if x is None else gr.Button(interactive=True), inputs=input_audio_ui, outputs=submit_audio_btn)\n",
    "\n",
    "    # block buttons, do the transcription and conversation, clear audio, unblock buttons\n",
    "    submit_audio_btn.click(lambda: gr.Button(interactive=False), outputs=submit_audio_btn) \\\n",
    "        .then(lambda: gr.Button(interactive=False), outputs=summarize_button)\\\n",
    "        .then(transcribe, inputs=[input_audio_ui, chatbot_ui], outputs=chatbot_ui)\\\n",
    "        .then(chat, chatbot_ui, chatbot_ui)\\\n",
    "        .then(lambda: None, inputs=[], outputs=[input_audio_ui])\\\n",
    "        .then(lambda: gr.Button(interactive=True), outputs=summarize_button)\n",
    "\n",
    "    # block button, do the summarization, unblock button\n",
    "    summarize_button.click(lambda: gr.Button(interactive=False), outputs=summarize_button) \\\n",
    "        .then(summarize, inputs=chatbot_ui, outputs=summary_ui) \\\n",
    "        .then(lambda: gr.Button(interactive=True), outputs=summarize_button)\n",
    "\n",
    "demo.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
